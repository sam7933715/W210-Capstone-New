{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/blam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install bertopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# !pip install bertopic[visualization]\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# !pip install spacy\n",
    "import spacy\n",
    "\n",
    "# python -m spacy download en\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990275</th>\n",
       "      <td>H0RIamZu0B0Ei0P4aeh3sQ</td>\n",
       "      <td>qskILQ3k0I_qcCMI-k6_QQ</td>\n",
       "      <td>jals67o91gcrD4DC81Vk6w</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Latest addition to services from ICCU is Apple...</td>\n",
       "      <td>2014-12-17 21:45:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990276</th>\n",
       "      <td>shTPgbgdwTHSuU67mGCmZQ</td>\n",
       "      <td>Zo0th2m8Ez4gLSbHftiQvg</td>\n",
       "      <td>2vLksaMmSEcGbjI5gywpZA</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>This spot offers a great, affordable east week...</td>\n",
       "      <td>2021-03-31 16:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990277</th>\n",
       "      <td>YNfNhgZlaaCO5Q_YJR4rEw</td>\n",
       "      <td>mm6E4FbCMwJmb7kPDZ5v2Q</td>\n",
       "      <td>R1khUUxidqfaJmcpmGd4aw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This Home Depot won me over when I needed to g...</td>\n",
       "      <td>2019-12-30 03:56:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990278</th>\n",
       "      <td>i-I4ZOhoX70Nw5H0FwrQUA</td>\n",
       "      <td>YwAMC-jvZ1fvEUum6QkEkw</td>\n",
       "      <td>Rr9kKArrMhSLVE9a53q-aA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>For when I'm feeling like ignoring my calorie-...</td>\n",
       "      <td>2022-01-19 18:59:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990279</th>\n",
       "      <td>RwcKOdEuLRHNJe4M9-qpqg</td>\n",
       "      <td>6JehEvdoCvZPJ_XIxnzIIw</td>\n",
       "      <td>VAeEXLbEcI9Emt9KGYq9aA</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Located in the 'Walking District' in Nashville...</td>\n",
       "      <td>2018-01-02 22:50:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990280 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "0        KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA   \n",
       "1        BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
       "2        saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A   \n",
       "3        AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "4        Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ   \n",
       "...                         ...                     ...   \n",
       "6990275  H0RIamZu0B0Ei0P4aeh3sQ  qskILQ3k0I_qcCMI-k6_QQ   \n",
       "6990276  shTPgbgdwTHSuU67mGCmZQ  Zo0th2m8Ez4gLSbHftiQvg   \n",
       "6990277  YNfNhgZlaaCO5Q_YJR4rEw  mm6E4FbCMwJmb7kPDZ5v2Q   \n",
       "6990278  i-I4ZOhoX70Nw5H0FwrQUA  YwAMC-jvZ1fvEUum6QkEkw   \n",
       "6990279  RwcKOdEuLRHNJe4M9-qpqg  6JehEvdoCvZPJ_XIxnzIIw   \n",
       "\n",
       "                    business_id  stars  useful  funny  cool  \\\n",
       "0        XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0   \n",
       "1        7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n",
       "2        YjUWPpI6HXG530lwP-fb2A      3       0      0     0   \n",
       "3        kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n",
       "4        e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1   \n",
       "...                         ...    ...     ...    ...   ...   \n",
       "6990275  jals67o91gcrD4DC81Vk6w      5       1      2     1   \n",
       "6990276  2vLksaMmSEcGbjI5gywpZA      5       2      1     2   \n",
       "6990277  R1khUUxidqfaJmcpmGd4aw      4       1      0     0   \n",
       "6990278  Rr9kKArrMhSLVE9a53q-aA      5       1      0     0   \n",
       "6990279  VAeEXLbEcI9Emt9KGYq9aA      3      10      3     7   \n",
       "\n",
       "                                                      text                date  \n",
       "0        If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
       "1        I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
       "2        Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
       "3        Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
       "4        Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
       "...                                                    ...                 ...  \n",
       "6990275  Latest addition to services from ICCU is Apple... 2014-12-17 21:45:20  \n",
       "6990276  This spot offers a great, affordable east week... 2021-03-31 16:55:10  \n",
       "6990277  This Home Depot won me over when I needed to g... 2019-12-30 03:56:30  \n",
       "6990278  For when I'm feeling like ignoring my calorie-... 2022-01-19 18:59:27  \n",
       "6990279  Located in the 'Walking District' in Nashville... 2018-01-02 22:50:47  \n",
       "\n",
       "[6990280 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Reviews\n",
    "reviews = pd.read_json(\n",
    "    \"/Users/blam/Documents/Datascience capstone/yelp_academic_dataset_review.json\",\n",
    "    lines=True,\n",
    ")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          If you decide to eat here, just be aware it is...\n",
       "1          I've taken a lot of spin classes over the year...\n",
       "2          Family diner. Had the buffet. Eclectic assortm...\n",
       "3          Wow!  Yummy, different,  delicious.   Our favo...\n",
       "4          Cute interior and owner (?) gave us tour of up...\n",
       "                                 ...                        \n",
       "6990275    Latest addition to services from ICCU is Apple...\n",
       "6990276    This spot offers a great, affordable east week...\n",
       "6990277    This Home Depot won me over when I needed to g...\n",
       "6990278    For when I'm feeling like ignoring my calorie-...\n",
       "6990279    Located in the 'Walking District' in Nashville...\n",
       "Name: text, Length: 6990280, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_text = reviews[\"text\"]\n",
    "reviews_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"from\", \"to\", \"how\", \"they\", \"very\", \"many\"])\n",
    "\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(\n",
    "    reviews_text, min_count=5, threshold=100\n",
    ")  # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[reviews_text], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(reviews_text)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_bigrams, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=6,\n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    per_word_topics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print(\n",
    "    \"\\nPerplexity: \", lda_model.log_perplexity(corpus)\n",
    ")  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score. Higher the topic coherence, the topic is more human interpretable.\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib inline\n",
    "!pip install pyLDAvis==2.1.2\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=id2word)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = BERTopic(verbose=True, nr_topics=5)\n",
    "\n",
    "# convert to list\n",
    "docs = reviews_text.to_list()\n",
    "\n",
    "topics, probabilities = model.fit_transform(docs)\n",
    "\n",
    "# Select top topics\n",
    "print(model.get_topic_freq().head(11))\n",
    "print(model.get_topic(6))\n",
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=8, n_words=20, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_hierarchy(height=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c055120e90413c7a6426da43687fc843cb3640cbfc6a57413de5a25d524f439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
